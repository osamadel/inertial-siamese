{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlFBAUNaKdXZ"
   },
   "source": [
    "# ICASSP Experiments\n",
    "\n",
    "Datasets : [MMUISD]\n",
    "\n",
    "**Data Variations**\n",
    "* Features = [Accelerometer Only, Accelerometer + Gyroscope]\n",
    "* Sequence Length = [1, 2, 3, 4]\n",
    "* Overlap = [no overlap, 0.25, 0.5, 0.75]\n",
    "* Train-Validation-Test split = [0.8, 0.1, 0.1]\n",
    "\n",
    "**Model Variations**\n",
    "* Dropout = [Yes, No]\n",
    "* BN = [Yes, No]\n",
    "* Weight Decay = [Yes, No]\n",
    "* Weight Decay + BN\n",
    "* Weight Decay + Dropout\n",
    "\n",
    "**Report:**\n",
    "* Mean EER\n",
    "* Std EER\n",
    "* Min EER\n",
    "* Max EER\n",
    "\n",
    "**Visualization**\n",
    "* Distance Vectors of Best and Worst combinations of all datasets stored in 200 ppt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 19822,
     "status": "ok",
     "timestamp": 1602713318601,
     "user": {
      "displayName": "Osama Feshier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgtCHGDwASgsNGXYj4hjnfcfmeFvO0oaORVbi50LQ=s64",
      "userId": "16137837750643399444"
     },
     "user_tz": -120
    },
    "id": "OI-lQFRRKEAY",
    "outputId": "ae1a9b6a-a0e4-403e-eec6-e783fa6c867b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 9627,
     "status": "ok",
     "timestamp": 1602713327679,
     "user": {
      "displayName": "Osama Feshier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgtCHGDwASgsNGXYj4hjnfcfmeFvO0oaORVbi50LQ=s64",
      "userId": "16137837750643399444"
     },
     "user_tz": -120
    },
    "id": "jArT8WzDKPH2",
    "outputId": "726d15f3-c6e4-4399-e21c-0ea0626cfa9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze.py  datasets.py  drive\tmodels.py  sample_data\tsrc\n"
     ]
    }
   ],
   "source": [
    "!cp -r \"/content/drive/My Drive/icmla2020/src\" /content/\n",
    "!mv src/datasets.py datasets.py\n",
    "!mv src/models.py models.py\n",
    "!mv src/analyze.py analyze.py\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/ejust'\n",
    "results_path = '../results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k0TFGFVAKTNo"
   },
   "outputs": [],
   "source": [
    "import datasets, models, analyze\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import activations\n",
    "# from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "# import nevergrad as ng\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jvrnb1vgKVPf"
   },
   "outputs": [],
   "source": [
    "def prepare_data(train_sps, val_sps, test_sps, max_cls, split, data, labels, indices_list, run_n):\n",
    "    train_split = int(split[0] * max_cls)\n",
    "    val_split = int(split[2] * max_cls)\n",
    "\n",
    "    rand_indices = np.random.choice(range(max_cls), size=max_cls, replace=False)\n",
    "    indices_list.append(rand_indices)\n",
    "    print('Run #%d:'%run_n)\n",
    "    \n",
    "    all_data_rand, all_labels_rand = [data[x] for x in rand_indices], [labels[x] for x in rand_indices]\n",
    "    b_data, b_labels = datasets.generate_batch(train_sps, all_data_rand[:train_split], all_labels_rand[:train_split])\n",
    "    \n",
    "    val_data, val_labels = datasets.generate_batch(val_sps, all_data_rand[train_split:train_split+val_split], all_labels_rand[train_split:train_split+val_split])\n",
    "    ridx = np.random.choice(range(b_data[0].shape[0]), size=b_data[0].shape[0], replace=False)\n",
    "    \n",
    "    b_data_test, b_labels_test = datasets.generate_batch(test_sps, all_data_rand[train_split+val_split:], all_labels_rand[train_split+val_split:])\n",
    "\n",
    "    l_input = b_data[0][ridx]\n",
    "    r_input = b_data[1][ridx]\n",
    "    b_labels = b_labels[ridx]\n",
    "\n",
    "    # print(l_input[0].shape)\n",
    "\n",
    "    l_input_val = val_data[0]\n",
    "    r_input_val = val_data[1]\n",
    "\n",
    "    l_input_test = b_data_test[0]\n",
    "    r_input_test = b_data_test[1]\n",
    "    return l_input, r_input, b_labels, l_input_val, r_input_val, val_labels, l_input_test, r_input_test, b_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JYx3YqoxK4PG"
   },
   "outputs": [],
   "source": [
    "def load_dataset(segLen, overlap, acc_only):\n",
    "    data, labels = datasets.load_segment_EJUST(data_path, 'D5-LC', [0, 20], segment_time=2)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-h4dUZV8WXlb"
   },
   "outputs": [],
   "source": [
    "def build_model1(input_shape, bn, reg):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv1D, \\\n",
    "                                        MaxPooling1D, \\\n",
    "                                        AveragePooling1D, \\\n",
    "                                        Dense, Flatten, Dropout, \\\n",
    "                                        BatchNormalization, \\\n",
    "                                        GlobalMaxPooling1D, \\\n",
    "                                        GlobalAveragePooling1D, \\\n",
    "                                        Activation\n",
    "    from tensorflow.keras import activations\n",
    "    import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "    model = Sequential()\n",
    "    if reg:\n",
    "        model.add(Conv1D(16, 3, strides=1, input_shape=input_shape, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(16, 3, strides=1, input_shape=input_shape))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    if reg:\n",
    "        model.add(Conv1D(64, 5, strides=2, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(64, 5, strides=2))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Auc0ROjJXJ4a"
   },
   "outputs": [],
   "source": [
    "def build_model2(input_shape, bn, reg):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv1D, \\\n",
    "                                        MaxPooling1D, \\\n",
    "                                        AveragePooling1D, \\\n",
    "                                        Dense, Flatten, Dropout, \\\n",
    "                                        BatchNormalization, \\\n",
    "                                        GlobalMaxPooling1D, \\\n",
    "                                        GlobalAveragePooling1D, \\\n",
    "                                        Activation\n",
    "    from tensorflow.keras import activations\n",
    "    import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "    model = Sequential()\n",
    "    if reg:\n",
    "        model.add(Conv1D(16, 3, strides=1, activation='tanh', input_shape=input_shape, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
    "        model.add(Conv1D(32, 3, strides=2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(16, 3, strides=1, activation='tanh', input_shape=input_shape))\n",
    "        model.add(Conv1D(32, 3, strides=2, activation='relu'))\n",
    "    \n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    if reg:\n",
    "        model.add(Conv1D(64, 5, strides=2, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
    "        model.add(Conv1D(128, 5, strides=3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(64, 5, strides=2, activation='tanh'))\n",
    "        model.add(Conv1D(128, 5, strides=3, activation='relu'))\n",
    "    \n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1S9BaZbCZajy"
   },
   "outputs": [],
   "source": [
    "def build_model3(input_shape, bn, reg):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv1D, \\\n",
    "                                        MaxPooling1D, \\\n",
    "                                        AveragePooling1D, \\\n",
    "                                        Dense, Flatten, Dropout, \\\n",
    "                                        BatchNormalization, \\\n",
    "                                        GlobalMaxPooling1D, \\\n",
    "                                        GlobalAveragePooling1D, \\\n",
    "                                        Activation\n",
    "    from tensorflow.keras import activations\n",
    "    import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 5, strides=3, padding='valid', activation='relu', input_shape=input_shape))\n",
    "    if bn: model.add(BatchNormalization())\n",
    "    model.add(Conv1D(128, 3, strides=2, padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(128, 2, strides=1, padding='valid', activation='tanh'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "q7x9rfwDIqx-"
   },
   "outputs": [],
   "source": [
    "def build_model4(input_shape, bn, reg):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv1D, \\\n",
    "                                        MaxPooling1D, \\\n",
    "                                        AveragePooling1D, \\\n",
    "                                        Dense, Flatten, Dropout, \\\n",
    "                                        BatchNormalization, \\\n",
    "                                        GlobalMaxPooling1D, \\\n",
    "                                        GlobalAveragePooling1D, \\\n",
    "                                        Activation\n",
    "    from tensorflow.keras import activations\n",
    "    import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "    model = Sequential()\n",
    "    if reg:\n",
    "        model.add(Conv1D(32, 3, strides=1, input_shape=input_shape, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(32, 3, strides=1, input_shape=input_shape))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    if reg:\n",
    "        model.add(Conv1D(128, 5, strides=2, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(128, 5, strides=2))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "F28ig3xVIzuJ"
   },
   "outputs": [],
   "source": [
    "def build_model5(input_shape, bn, reg):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv1D, \\\n",
    "                                        MaxPooling1D, \\\n",
    "                                        AveragePooling1D, \\\n",
    "                                        Dense, Flatten, Dropout, \\\n",
    "                                        BatchNormalization, \\\n",
    "                                        GlobalMaxPooling1D, \\\n",
    "                                        GlobalAveragePooling1D, \\\n",
    "                                        Activation\n",
    "    from tensorflow.keras import activations\n",
    "    import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "    model = Sequential()\n",
    "    if reg:\n",
    "        model.add(Conv1D(64, 3, strides=1, input_shape=input_shape, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(64, 3, strides=1, input_shape=input_shape))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    if reg:\n",
    "        model.add(Conv1D(256, 5, strides=2, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(256, 5, strides=2))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DFUOb5_GUo-e"
   },
   "outputs": [],
   "source": [
    "acc_only_list    = [False, True]\n",
    "segLen_list      = [1, 2]\n",
    "overlap_list     = [0, 0.25, 0.5, 0.75]\n",
    "# models_list      = [build_model1, build_model2, build_model3]\n",
    "models_list      = [build_model4, build_model5]\n",
    "bn_list          = [True, False]\n",
    "reg_list         = [True, False]\n",
    "\n",
    "# Configurations\n",
    "train_sps       = 500\n",
    "val_sps         = 200\n",
    "test_sps        = 200\n",
    "max_cls         = 20\n",
    "split           = [0.5, 0.25, 0.25]\n",
    "output_dropout  = 0.1\n",
    "runs            = 30\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XqrMb0KjbKDW",
    "outputId": "9d5685c6-9f54-40da-91d0-e20ed29bf010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #0:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 98, 32)            608       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 98, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 49, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 23, 128)           20608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 23, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 23, 128)           0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 21,856\n",
      "Trainable params: 21,536\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "Train on 10000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 - 4s - loss: 0.2974 - accuracy: 0.9032 - val_loss: 0.5062 - val_accuracy: 0.7660\n",
      "Epoch 2/100\n",
      "10000/10000 - 1s - loss: 0.1220 - accuracy: 0.9787 - val_loss: 0.7402 - val_accuracy: 0.6705\n",
      "Epoch 3/100\n",
      "10000/10000 - 1s - loss: 0.0676 - accuracy: 0.9919 - val_loss: 0.5986 - val_accuracy: 0.7350\n",
      "Epoch 4/100\n",
      "10000/10000 - 1s - loss: 0.0463 - accuracy: 0.9958 - val_loss: 0.4941 - val_accuracy: 0.7825\n",
      "Epoch 5/100\n",
      "10000/10000 - 1s - loss: 0.0347 - accuracy: 0.9984 - val_loss: 0.4738 - val_accuracy: 0.7885\n",
      "Epoch 6/100\n",
      "10000/10000 - 1s - loss: 0.0287 - accuracy: 0.9983 - val_loss: 0.5423 - val_accuracy: 0.7895\n",
      "Epoch 7/100\n",
      "10000/10000 - 1s - loss: 0.0267 - accuracy: 0.9992 - val_loss: 0.5682 - val_accuracy: 0.7810\n",
      "Epoch 8/100\n",
      "10000/10000 - 1s - loss: 0.0244 - accuracy: 0.9991 - val_loss: 0.5484 - val_accuracy: 0.7830\n",
      "Epoch 9/100\n",
      "10000/10000 - 1s - loss: 0.0232 - accuracy: 0.9987 - val_loss: 0.6243 - val_accuracy: 0.7665\n",
      "Epoch 10/100\n",
      "10000/10000 - 1s - loss: 0.0230 - accuracy: 0.9991 - val_loss: 0.5746 - val_accuracy: 0.7990\n",
      "Epoch 11/100\n",
      "10000/10000 - 1s - loss: 0.0330 - accuracy: 0.9960 - val_loss: 0.3542 - val_accuracy: 0.8545\n",
      "Epoch 12/100\n",
      "10000/10000 - 1s - loss: 0.0217 - accuracy: 0.9997 - val_loss: 0.4766 - val_accuracy: 0.8215\n",
      "Epoch 13/100\n",
      "10000/10000 - 1s - loss: 0.0240 - accuracy: 0.9986 - val_loss: 0.5356 - val_accuracy: 0.8150\n",
      "Epoch 14/100\n",
      "10000/10000 - 1s - loss: 0.0217 - accuracy: 0.9990 - val_loss: 0.6372 - val_accuracy: 0.7860\n",
      "Epoch 15/100\n",
      "10000/10000 - 1s - loss: 0.0205 - accuracy: 0.9991 - val_loss: 0.6888 - val_accuracy: 0.7995\n",
      "Epoch 16/100\n",
      "10000/10000 - 1s - loss: 0.0203 - accuracy: 0.9991 - val_loss: 0.5653 - val_accuracy: 0.8220\n"
     ]
    }
   ],
   "source": [
    "data, labels = load_dataset(2, 0, acc_only=False)\n",
    "\n",
    "# Results of this configuration/experiment\n",
    "EERs = np.array([np.zeros(runs) for _ in range(3*2*2)])\n",
    "\n",
    "for run_n in range(runs):\n",
    "    l_input, r_input, b_labels, l_input_val, r_input_val, val_labels, l_input_test, r_input_test, b_labels_test = prepare_data(train_sps,\n",
    "                                                                                                                                val_sps,\n",
    "                                                                                                                                test_sps,\n",
    "                                                                                                                                max_cls,\n",
    "                                                                                                                                split,\n",
    "                                                                                                                                data,\n",
    "                                                                                                                                labels,\n",
    "                                                                                                                                [],\n",
    "                                                                                                                                run_n)\n",
    "    adam = Adam(learning_rate=0.001)\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "    for bn in bn_list:\n",
    "        for reg in reg_list:\n",
    "            for model_index, build_cnn in enumerate(models_list):\n",
    "                cnn = build_cnn(l_input[0].shape, bn, reg)\n",
    "                siamese = models.build_siamese(l_input[0].shape, cnn, output_dropout)\n",
    "                siamese.compile(loss='binary_crossentropy', optimizer=adam, metrics=[BinaryAccuracy(name='accuracy')])\n",
    "                if run_n == 0: \n",
    "                    cnn.summary()\n",
    "                hist = siamese.fit([l_input, r_input], \n",
    "                                    b_labels, \n",
    "                                    shuffle=True,\n",
    "                                    batch_size=64,\n",
    "                                    epochs=100,\n",
    "                                    callbacks=[earlystop],\n",
    "                                    validation_data=([l_input_val, r_input_val], val_labels),\n",
    "                                    verbose=2\n",
    "                                    )\n",
    "                FRR, FAR, EER, EER_th = analyze.ROC(siamese, [l_input_test, r_input_test], b_labels_test)\n",
    "                print('MODEL(%d) BN(%d) REG(%d) EER: %g', (model_index, bn, reg, EER))\n",
    "                EERs[4 * model_index + 2*bn + reg , run_n] = EER\n",
    "                np.save(results_path + 'ejust_filters_eers.npy', EERs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhRPrSfTU_53"
   },
   "outputs": [],
   "source": [
    "EERs = np.load(results_path + 'ejust_filters_eers.npy')\n",
    "print(EERs[11,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XxyoFzLDg8I"
   },
   "outputs": [],
   "source": [
    "# EERs[EERs == 0] = 100\n",
    "means = np.mean(EERs, axis=1) * 100\n",
    "print(means)\n",
    "\n",
    "stds = np.std(EERs, axis=1) * 100\n",
    "print(stds)\n",
    "\n",
    "mins = np.min(EERs, axis=1) * 100\n",
    "print(mins)\n",
    "\n",
    "mins = np.max(EERs, axis=1) * 100\n",
    "print(mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AFFebUvDhys"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "E = pd.DataFrame(EERs)\n",
    "print(E.iloc[:,:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gVrpFPdIUnQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOhtDhaxUd0J1W3B0DGq1R3",
   "collapsed_sections": [],
   "name": "ejust_experiments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
