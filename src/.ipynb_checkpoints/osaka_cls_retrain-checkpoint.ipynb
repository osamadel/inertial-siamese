{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:tf2]",
      "language": "python",
      "name": "conda-env-tf2-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "osaka_cls_retrain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvAor71-TFxU"
      },
      "source": [
        "# IJCNN Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DBFcSs6TFxe"
      },
      "source": [
        "Roughly speaking, I will try to use the models trained on the datasets, then remove the FC layer and add other classifiers. Then I will report the result of the other classifiers on the final performance using either EER or accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOPS399rTFxe"
      },
      "source": [
        "# data_path = '../data/osaka'\n",
        "# results_path = '../results/'\n",
        "\n",
        "data_path = '/content/drive/MyDrive/IJCNN/data/osaka'\n",
        "\n",
        "\n",
        "results_path = '/content/drive/MyDrive/IJCNN/results'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI-lQFRRKEAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e85a733-61b5-4742-a5c2-2ed5b6502d45"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jArT8WzDKPH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e463a28e-25ed-4c96-870b-9cfec0a36b87"
      },
      "source": [
        "!cp -r \"/content/drive/MyDrive/IJCNN/src\" /content/\n",
        "!mv src/datasets.py datasets.py\n",
        "!mv src/models.py models.py\n",
        "!mv src/analyze.py analyze.py\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze.py  datasets.py  drive\tmodels.py  sample_data\tsrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0TFGFVAKTNo"
      },
      "source": [
        "import datasets, models, analyze\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import activations\n",
        "# from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "# import nevergrad as ng\n",
        "import os\n",
        "np.set_printoptions(precision=4)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwqf4RGPJ08n"
      },
      "source": [
        "from tensorflow.keras import Model\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvrnb1vgKVPf"
      },
      "source": [
        "def prepare_data(train_sps, val_sps, test_sps, max_cls, split, data, labels, indices_list, run_n):\n",
        "    train_split = int(split[0] * max_cls)\n",
        "    val_split = int(split[2] * max_cls)\n",
        "\n",
        "    rand_indices = np.random.choice(range(max_cls), size=max_cls, replace=False)\n",
        "    indices_list.append(rand_indices)\n",
        "    print('Run #%d:'%run_n)\n",
        "    \n",
        "    all_data_rand, all_labels_rand = [data[x] for x in rand_indices], [labels[x] for x in rand_indices]\n",
        "    b_data, b_labels = datasets.generate_batch(train_sps, all_data_rand[:train_split], all_labels_rand[:train_split])\n",
        "    \n",
        "    val_data, val_labels = datasets.generate_batch(val_sps, all_data_rand[train_split:train_split+val_split], all_labels_rand[train_split:train_split+val_split])\n",
        "    ridx = np.random.choice(range(b_data[0].shape[0]), size=b_data[0].shape[0], replace=False)\n",
        "    \n",
        "    b_data_test, b_labels_test = datasets.generate_batch(test_sps, all_data_rand[train_split+val_split:], all_labels_rand[train_split+val_split:])\n",
        "\n",
        "    l_input = b_data[0][ridx]\n",
        "    r_input = b_data[1][ridx]\n",
        "    b_labels = b_labels[ridx]\n",
        "\n",
        "    # print(l_input[0].shape)\n",
        "\n",
        "    l_input_val = val_data[0]\n",
        "    r_input_val = val_data[1]\n",
        "\n",
        "    l_input_test = b_data_test[0]\n",
        "    r_input_test = b_data_test[1]\n",
        "    return l_input, r_input, b_labels, l_input_val, r_input_val, val_labels, l_input_test, r_input_test, b_labels_test"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYx3YqoxK4PG"
      },
      "source": [
        "def load_dataset(segLen, overlap, acc_only):\n",
        "    data, labels = datasets.load_segment_osaka(data_path, \n",
        "                                            [0,744], \n",
        "                                            sample_rate=100,\n",
        "                                            acc_only=acc_only,\n",
        "                                            segment_time=segLen, \n",
        "                                            overlapped=overlap, \n",
        "                                            overlap=overlap, \n",
        "                                            downsample=True)\n",
        "    return data, labels"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h4dUZV8WXlb"
      },
      "source": [
        "def build_model1(input_shape, bn, reg):\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv1D, \\\n",
        "                                        MaxPooling1D, \\\n",
        "                                        AveragePooling1D, \\\n",
        "                                        Dense, Flatten, Dropout, \\\n",
        "                                        BatchNormalization, \\\n",
        "                                        GlobalMaxPooling1D, \\\n",
        "                                        GlobalAveragePooling1D, \\\n",
        "                                        Activation\n",
        "    from tensorflow.keras import activations\n",
        "    import tensorflow.keras.regularizers as regularizers\n",
        "\n",
        "    model = Sequential()\n",
        "    if reg:\n",
        "        model.add(Conv1D(16, 3, strides=1, input_shape=input_shape, \n",
        "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "                        bias_regularizer=regularizers.l2(1e-4)))\n",
        "    else:\n",
        "        model.add(Conv1D(16, 3, strides=1, input_shape=input_shape))\n",
        "    if bn:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation(activations.relu))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    if reg:\n",
        "        model.add(Conv1D(64, 5, strides=2, \n",
        "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "                        bias_regularizer=regularizers.l2(1e-4)))\n",
        "    else:\n",
        "        model.add(Conv1D(64, 5, strides=2))\n",
        "    if bn:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation(activations.relu))\n",
        "    model.add(AveragePooling1D(pool_size=2))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auc0ROjJXJ4a"
      },
      "source": [
        "# def build_model2(input_shape, bn, reg):\n",
        "#     from tensorflow.keras.models import Sequential\n",
        "#     from tensorflow.keras.layers import Conv1D, \\\n",
        "#                                         MaxPooling1D, \\\n",
        "#                                         AveragePooling1D, \\\n",
        "#                                         Dense, Flatten, Dropout, \\\n",
        "#                                         BatchNormalization, \\\n",
        "#                                         GlobalMaxPooling1D, \\\n",
        "#                                         GlobalAveragePooling1D, \\\n",
        "#                                         Activation\n",
        "#     from tensorflow.keras import activations\n",
        "#     import tensorflow.keras.regularizers as regularizers\n",
        "\n",
        "#     model = Sequential()\n",
        "#     if reg:\n",
        "#         model.add(Conv1D(16, 3, strides=1, activation='tanh', input_shape=input_shape, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
        "#         model.add(Conv1D(32, 3, strides=2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
        "#     else:\n",
        "#         model.add(Conv1D(16, 3, strides=1, activation='tanh', input_shape=input_shape))\n",
        "#         model.add(Conv1D(32, 3, strides=2, activation='relu'))\n",
        "    \n",
        "#     if bn:\n",
        "#         model.add(BatchNormalization())\n",
        "\n",
        "#     model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#     if reg:\n",
        "#         model.add(Conv1D(64, 5, strides=2, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
        "#         model.add(Conv1D(128, 5, strides=3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
        "#     else:\n",
        "#         model.add(Conv1D(64, 5, strides=2, activation='tanh'))\n",
        "#         model.add(Conv1D(128, 5, strides=3, activation='relu'))\n",
        "    \n",
        "#     if bn:\n",
        "#         model.add(BatchNormalization())\n",
        "\n",
        "#     model.add(GlobalMaxPooling1D())\n",
        "#     return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S9BaZbCZajy"
      },
      "source": [
        "# def build_model3(input_shape, bn, reg):\n",
        "#     from tensorflow.keras.models import Sequential\n",
        "#     from tensorflow.keras.layers import Conv1D, \\\n",
        "#                                         MaxPooling1D, \\\n",
        "#                                         AveragePooling1D, \\\n",
        "#                                         Dense, Flatten, Dropout, \\\n",
        "#                                         BatchNormalization, \\\n",
        "#                                         GlobalMaxPooling1D, \\\n",
        "#                                         GlobalAveragePooling1D, \\\n",
        "#                                         Activation\n",
        "#     from tensorflow.keras import activations\n",
        "#     import tensorflow.keras.regularizers as regularizers\n",
        "\n",
        "#     model = Sequential()\n",
        "#     model.add(Conv1D(64, 5, strides=3, padding='valid', activation='relu', input_shape=input_shape))\n",
        "#     if bn: model.add(BatchNormalization())\n",
        "#     model.add(Conv1D(128, 3, strides=2, padding='valid', activation='relu'))\n",
        "#     model.add(MaxPooling1D(pool_size=2, strides=2))\n",
        "#     model.add(Conv1D(128, 2, strides=1, padding='valid', activation='tanh'))\n",
        "#     model.add(MaxPooling1D(pool_size=2, strides=2))\n",
        "#     model.add(GlobalMaxPooling1D())\n",
        "#     return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7x9rfwDIqx-"
      },
      "source": [
        "def build_model4(input_shape, bn, reg):\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv1D, \\\n",
        "                                        MaxPooling1D, \\\n",
        "                                        AveragePooling1D, \\\n",
        "                                        Dense, Flatten, Dropout, \\\n",
        "                                        BatchNormalization, \\\n",
        "                                        GlobalMaxPooling1D, \\\n",
        "                                        GlobalAveragePooling1D, \\\n",
        "                                        Activation\n",
        "    from tensorflow.keras import activations\n",
        "    import tensorflow.keras.regularizers as regularizers\n",
        "\n",
        "    model = Sequential()\n",
        "    if reg:\n",
        "        model.add(Conv1D(32, 3, strides=1, input_shape=input_shape, \n",
        "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "                        bias_regularizer=regularizers.l2(1e-4)))\n",
        "    else:\n",
        "        model.add(Conv1D(32, 3, strides=1, input_shape=input_shape))\n",
        "    if bn:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation(activations.relu))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    if reg:\n",
        "        model.add(Conv1D(128, 5, strides=2, \n",
        "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "                        bias_regularizer=regularizers.l2(1e-4)))\n",
        "    else:\n",
        "        model.add(Conv1D(128, 5, strides=2))\n",
        "    if bn:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation(activations.relu))\n",
        "    model.add(AveragePooling1D(pool_size=2))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F28ig3xVIzuJ"
      },
      "source": [
        "def build_model5(input_shape, bn, reg):\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv1D, \\\n",
        "                                        MaxPooling1D, \\\n",
        "                                        AveragePooling1D, \\\n",
        "                                        Dense, Flatten, Dropout, \\\n",
        "                                        BatchNormalization, \\\n",
        "                                        GlobalMaxPooling1D, \\\n",
        "                                        GlobalAveragePooling1D, \\\n",
        "                                        Activation\n",
        "    from tensorflow.keras import activations\n",
        "    import tensorflow.keras.regularizers as regularizers\n",
        "\n",
        "    model = Sequential()\n",
        "    if reg:\n",
        "        model.add(Conv1D(64, 3, strides=1, input_shape=input_shape, \n",
        "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "                        bias_regularizer=regularizers.l2(1e-4)))\n",
        "    else:\n",
        "        model.add(Conv1D(64, 3, strides=1, input_shape=input_shape))\n",
        "    if bn:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation(activations.relu))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    if reg:\n",
        "        model.add(Conv1D(256, 5, strides=2, \n",
        "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
        "                        bias_regularizer=regularizers.l2(1e-4)))\n",
        "    else:\n",
        "        model.add(Conv1D(256, 5, strides=2))\n",
        "    if bn:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation(activations.relu))\n",
        "    model.add(AveragePooling1D(pool_size=2))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFUOb5_GUo-e"
      },
      "source": [
        "acc_only_list    = [False, True]\n",
        "segLen_list      = [1, 2]\n",
        "overlap_list     = [0, 0.25, 0.5, 0.75]\n",
        "# models_list      = [build_model1, build_model2, build_model3]\n",
        "models_list      = [build_model5]\n",
        "# bn_list          = [True, False]\n",
        "bn_list          = [True]\n",
        "# reg_list         = [True, False]\n",
        "reg_list         = [False]\n",
        "\n",
        "# Configurations\n",
        "train_sps       = 50\n",
        "val_sps         = 20\n",
        "test_sps        = 20\n",
        "max_cls         = 744\n",
        "split           = [0.8, 0.1, 0.1]\n",
        "output_dropout  = 0.1\n",
        "runs            = 3\n",
        "np.set_printoptions(precision=4)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxkqRdMpDOip"
      },
      "source": [
        "def getSiameseAcc(siamese, test_set, EER_th):\r\n",
        "    # l_input_test, r_input_test = test_set[0]\r\n",
        "    b_labels_test = test_set[1]\r\n",
        "    # print('EER threshold:', EER_th)\r\n",
        "    # print(np.uint8(np.squeeze(siamese.predict(test_set[0])) >= EER_th))\r\n",
        "    # print(b_labels_test)\r\n",
        "    return np.mean(np.uint8(np.squeeze(siamese.predict(test_set[0])) >= EER_th) == b_labels_test)\r\n",
        "\r\n",
        "def trainSVM(siamese, training_set, test_set):\r\n",
        "    l_input, r_input = training_set[0]\r\n",
        "    b_labels = training_set[1]\r\n",
        "    l_input_test, r_input_test = test_set[0]\r\n",
        "    b_labels_test = test_set[1]\r\n",
        "\r\n",
        "    # classifier = SVC(kernel='poly', degree=10, C=100)\r\n",
        "    classifier = SVC(kernel='poly', degree=5, C=10)\r\n",
        "    feature_exctractor = Model(inputs=[siamese.get_layer('left_input').input,siamese.get_layer('right_input').input], outputs=siamese.get_layer('lambda').output)\r\n",
        "    d_vect = feature_exctractor.predict([l_input, r_input])\r\n",
        "    d_vect_test = feature_exctractor.predict([l_input_test, r_input_test])\r\n",
        "    \r\n",
        "    # print(d_vect.shape)\r\n",
        "\r\n",
        "    classifier.fit(d_vect, b_labels)\r\n",
        "    svc_score = classifier.score(d_vect_test, b_labels_test)\r\n",
        "    # print('SVC mean test accuracy:', svc_score*100)\r\n",
        "    return svc_score\r\n",
        "\r\n",
        "def trainKNN(siamese, training_set, test_set):\r\n",
        "    l_input, r_input = training_set[0]\r\n",
        "    b_labels = training_set[1]\r\n",
        "    l_input_test, r_input_test = test_set[0]\r\n",
        "    b_labels_test = test_set[1]\r\n",
        "\r\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\r\n",
        "    feature_exctractor = Model(inputs=[siamese.get_layer('left_input').input,siamese.get_layer('right_input').input], outputs=siamese.get_layer('lambda').output)\r\n",
        "    d_vect = feature_exctractor.predict([l_input, r_input])\r\n",
        "    d_vect_test = feature_exctractor.predict([l_input_test, r_input_test])\r\n",
        "\r\n",
        "    knn.fit(d_vect, b_labels)\r\n",
        "    test_predictions = knn.predict(d_vect_test)\r\n",
        "    knn_score = knn.score(d_vect_test, b_labels_test)\r\n",
        "    # print('KNN mean test accuracy:', knn_score*100)\r\n",
        "    return knn_score\r\n",
        "\r\n",
        "def trainANN(siamese, training_set, test_set):\r\n",
        "    from tensorflow.keras.models import Sequential\r\n",
        "    from tensorflow.keras.layers import Dense, Dropout\r\n",
        "\r\n",
        "    l_input, r_input = training_set[0]\r\n",
        "    b_labels = training_set[1]\r\n",
        "    l_input_test, r_input_test = test_set[0]\r\n",
        "    b_labels_test = test_set[1]\r\n",
        "\r\n",
        "    feature_exctractor = Model(inputs=[siamese.get_layer('left_input').input,siamese.get_layer('right_input').input], outputs=siamese.get_layer('lambda').output)\r\n",
        "    d_vect = feature_exctractor.predict([l_input, r_input])\r\n",
        "    d_vect_test = feature_exctractor.predict([l_input_test, r_input_test])\r\n",
        "\r\n",
        "    classifier = Sequential()\r\n",
        "    classifier.add(Dense(8, activation='relu', input_shape=(64,)))\r\n",
        "    classifier.add(Dropout(0.3))\r\n",
        "    classifier.add(Dense(8, activation='relu'))\r\n",
        "    classifier.add(Dropout(0.3))\r\n",
        "    classifier.add(Dense(1, activation='sigmoid'))\r\n",
        "    classifier.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\r\n",
        "\r\n",
        "    classifier.fit(d_vect, b_labels, verbose=0, epochs=20, batch_size=128)\r\n",
        "    # test_predictions = np.uint8(np.squeeze(classifier.predict(d_vect_test) >= 0.5))\r\n",
        "    # print(test_predictions[:10])\r\n",
        "    ann_score = classifier.evaluate(d_vect_test, b_labels_test)[1]\r\n",
        "    # print('ANN mean test accuracy:', ann_score*100)\r\n",
        "    return ann_score"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-z-EqS3TFxl"
      },
      "source": [
        "def one_run(train_sps, val_sps, test_sps, max_cls, split, data, labels, lst, run_n):\n",
        "    accuracies = {}\n",
        "    l_input, r_input, b_labels \\\n",
        "        , l_input_val, r_input_val, val_labels \\\n",
        "            , l_input_test, r_input_test, b_labels_test = prepare_data( train_sps,\n",
        "                                                                        val_sps,\n",
        "                                                                        test_sps,\n",
        "                                                                        max_cls,\n",
        "                                                                        split,\n",
        "                                                                        data,\n",
        "                                                                        labels,\n",
        "                                                                        [],\n",
        "                                                                        run_n)\n",
        "    print('Loaded data')\n",
        "    adam = Adam(learning_rate=0.001)\n",
        "    earlystop = EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "    \n",
        "    cnn = build_model5(l_input[0].shape, bn=1, reg=0)\n",
        "    siamese = models.build_siamese(l_input[0].shape, cnn, output_dropout)\n",
        "    siamese.compile(loss='binary_crossentropy', optimizer=adam, metrics=[BinaryAccuracy(name='accuracy')])\n",
        "    print('Built model')\n",
        "    if run_n == 0: \n",
        "        cnn.summary()\n",
        "    print('Start training ...')\n",
        "    hist = siamese.fit([l_input, r_input], \n",
        "                        b_labels, \n",
        "                        shuffle=True,\n",
        "                        batch_size=64,\n",
        "                        epochs=100,\n",
        "                        callbacks=[earlystop],\n",
        "                        validation_data=([l_input_val, r_input_val], val_labels),\n",
        "                        verbose=2\n",
        "                        )\n",
        "    print('Finished training')\n",
        "    siamese.save_weights(os.path.join(results_path, 'osaka', 'model_r{}_weights.h5'.format(run_n)))\n",
        "    print('Saved model')\n",
        "    FRR, FAR, EER, EER_th = analyze.ROC(siamese, [l_input_test, r_input_test], b_labels_test)\n",
        "    print('Siamese EER:', EER)\n",
        "    accuracies['siamese'] = getSiameseAcc(siamese, ([l_input_test, r_input_test], b_labels_test), EER_th)\n",
        "    print('Siamese Accuracy:', accuracies['siamese'])\n",
        "    accuracies['svm'] = trainSVM(siamese, ([l_input, r_input], b_labels), ([l_input_test, r_input_test], b_labels_test))\n",
        "    print('SVM Accuracy:', accuracies['svm'])\n",
        "    accuracies['ann'] = trainANN(siamese, ([l_input, r_input], b_labels), ([l_input_test, r_input_test], b_labels_test))\n",
        "    print('ANN Accuracy:', accuracies['ann'])\n",
        "    accuracies['knn'] = trainKNN(siamese, ([l_input, r_input], b_labels), ([l_input_test, r_input_test], b_labels_test))\n",
        "    print('KNN Accuracy:', accuracies['knn'])\n",
        "\n",
        "\n",
        "    return siamese, [FRR, FAR, EER, EER_th], accuracies"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7TKW9MyTFxl"
      },
      "source": [
        "### Experiment 1: Train Different Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hiy6B7cuTFxm",
        "outputId": "0d3b8a68-833f-49db-8694-01d1d4aa6146"
      },
      "source": [
        "data, labels = load_dataset(2, 0.75, acc_only=False)\n",
        "num_cls = 3 # number of classifiers\n",
        "total_accs = np.zeros((runs, num_cls))\n",
        "\n",
        "# Results of this configuration/experiment\n",
        "# EERs = np.array([np.zeros(runs) for _ in range(3*2*2)])\n",
        "\n",
        "for run_n in range(runs):\n",
        "    siamese, ROC, accs = one_run(train_sps, val_sps, test_sps, max_cls, split, data, labels, [], run_n)\n",
        "    for ii, c in enumerate(accs):\n",
        "        total_accs[run_n, ii] = accs[c]\n",
        "    \n",
        "    np.save(os.path.join(results_path, 'osaka', 'classifiers.npy'), total_accs)\n",
        "    \n",
        "#     EERs[4 * model_index + 2*bn + reg , run_n] = EER\n",
        "#     np.save(results_path + 'osaka_filters_eers2.npy', EERs)\n",
        "\n",
        "for ii, c in enumerate(accs):\n",
        "    print(total_accs[:,ii])\n",
        "    print('{} accuracy: {} +/- {}'.format(c, total_accs[:,ii].mean(), total_accs[:,ii].std()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run #0:\n",
            "Loaded data\n",
            "Built model\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_60 (Conv1D)           (None, 98, 64)            1216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 98, 64)            256       \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 98, 64)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_30 (MaxPooling (None, 49, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_61 (Conv1D)           (None, 23, 256)           82176     \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 23, 256)           1024      \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 23, 256)           0         \n",
            "_________________________________________________________________\n",
            "average_pooling1d_30 (Averag (None, 11, 256)           0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_30 (Glo (None, 256)               0         \n",
            "=================================================================\n",
            "Total params: 84,672\n",
            "Trainable params: 84,032\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n",
            "Start training ...\n",
            "Epoch 1/100\n",
            "930/930 - 7s - loss: 0.1739 - accuracy: 0.9415 - val_loss: 0.1046 - val_accuracy: 0.9659\n",
            "Epoch 2/100\n",
            "930/930 - 5s - loss: 0.0713 - accuracy: 0.9783 - val_loss: 0.0893 - val_accuracy: 0.9666\n",
            "Epoch 3/100\n",
            "930/930 - 5s - loss: 0.0462 - accuracy: 0.9861 - val_loss: 0.0680 - val_accuracy: 0.9733\n",
            "Epoch 4/100\n",
            "930/930 - 5s - loss: 0.0346 - accuracy: 0.9899 - val_loss: 0.0709 - val_accuracy: 0.9750\n",
            "Epoch 5/100\n",
            "930/930 - 5s - loss: 0.0263 - accuracy: 0.9925 - val_loss: 0.0851 - val_accuracy: 0.9689\n",
            "Epoch 6/100\n",
            "930/930 - 5s - loss: 0.0230 - accuracy: 0.9930 - val_loss: 0.0691 - val_accuracy: 0.9703\n",
            "Epoch 7/100\n",
            "930/930 - 5s - loss: 0.0220 - accuracy: 0.9932 - val_loss: 0.0754 - val_accuracy: 0.9720\n",
            "Epoch 8/100\n",
            "930/930 - 5s - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.0631 - val_accuracy: 0.9780\n",
            "Epoch 9/100\n",
            "930/930 - 5s - loss: 0.0173 - accuracy: 0.9946 - val_loss: 0.0650 - val_accuracy: 0.9797\n",
            "Epoch 10/100\n",
            "930/930 - 5s - loss: 0.0138 - accuracy: 0.9956 - val_loss: 0.0637 - val_accuracy: 0.9787\n",
            "Epoch 11/100\n",
            "930/930 - 5s - loss: 0.0143 - accuracy: 0.9953 - val_loss: 0.0726 - val_accuracy: 0.9733\n",
            "Epoch 12/100\n",
            "930/930 - 5s - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.0669 - val_accuracy: 0.9770\n",
            "Epoch 13/100\n",
            "930/930 - 5s - loss: 0.0137 - accuracy: 0.9960 - val_loss: 0.0656 - val_accuracy: 0.9753\n",
            "Epoch 14/100\n",
            "930/930 - 5s - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0713 - val_accuracy: 0.9767\n",
            "Finished training\n",
            "Saved model\n",
            "Siamese EER: 0.038\n",
            "Siamese Accuracy: 0.962\n",
            "SVM Accuracy: 0.9646666666666667\n",
            "KNN Accuracy: 0.9546666666666667\n",
            "Run #1:\n",
            "Loaded data\n",
            "Built model\n",
            "Start training ...\n",
            "Epoch 1/100\n",
            "930/930 - 7s - loss: 0.1728 - accuracy: 0.9416 - val_loss: 0.1032 - val_accuracy: 0.9669\n",
            "Epoch 2/100\n",
            "930/930 - 5s - loss: 0.0733 - accuracy: 0.9777 - val_loss: 0.1054 - val_accuracy: 0.9601\n",
            "Epoch 3/100\n",
            "930/930 - 5s - loss: 0.0489 - accuracy: 0.9852 - val_loss: 0.0816 - val_accuracy: 0.9723\n",
            "Epoch 4/100\n",
            "930/930 - 5s - loss: 0.0361 - accuracy: 0.9890 - val_loss: 0.0823 - val_accuracy: 0.9686\n",
            "Epoch 5/100\n",
            "930/930 - 5s - loss: 0.0299 - accuracy: 0.9910 - val_loss: 0.0776 - val_accuracy: 0.9706\n",
            "Epoch 6/100\n",
            "930/930 - 5s - loss: 0.0244 - accuracy: 0.9927 - val_loss: 0.0838 - val_accuracy: 0.9686\n",
            "Epoch 7/100\n",
            "930/930 - 5s - loss: 0.0205 - accuracy: 0.9934 - val_loss: 0.0808 - val_accuracy: 0.9703\n",
            "Epoch 8/100\n",
            "930/930 - 5s - loss: 0.0188 - accuracy: 0.9941 - val_loss: 0.0730 - val_accuracy: 0.9716\n",
            "Finished training\n",
            "Saved model\n",
            "Siamese EER: 0.020666666666666667\n",
            "Siamese Accuracy: 0.9793333333333333\n",
            "SVM Accuracy: 0.9666666666666667\n",
            "KNN Accuracy: 0.9656666666666667\n",
            "Run #2:\n",
            "Loaded data\n",
            "Built model\n",
            "Start training ...\n",
            "Epoch 1/100\n",
            "930/930 - 7s - loss: 0.1790 - accuracy: 0.9382 - val_loss: 0.1120 - val_accuracy: 0.9608\n",
            "Epoch 2/100\n",
            "930/930 - 5s - loss: 0.0742 - accuracy: 0.9773 - val_loss: 0.1109 - val_accuracy: 0.9588\n",
            "Epoch 3/100\n",
            "930/930 - 5s - loss: 0.0464 - accuracy: 0.9860 - val_loss: 0.0791 - val_accuracy: 0.9709\n",
            "Epoch 4/100\n",
            "930/930 - 5s - loss: 0.0345 - accuracy: 0.9898 - val_loss: 0.0829 - val_accuracy: 0.9706\n",
            "Epoch 5/100\n",
            "930/930 - 5s - loss: 0.0271 - accuracy: 0.9924 - val_loss: 0.0954 - val_accuracy: 0.9635\n",
            "Epoch 6/100\n",
            "930/930 - 5s - loss: 0.0217 - accuracy: 0.9937 - val_loss: 0.0845 - val_accuracy: 0.9659\n",
            "Epoch 7/100\n",
            "930/930 - 5s - loss: 0.0212 - accuracy: 0.9935 - val_loss: 0.0872 - val_accuracy: 0.9706\n",
            "Epoch 8/100\n",
            "930/930 - 5s - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.0789 - val_accuracy: 0.9693\n",
            "Finished training\n",
            "Saved model\n",
            "Siamese EER: 0.03333333333333333\n",
            "Siamese Accuracy: 0.9666666666666667\n",
            "SVM Accuracy: 0.9653333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDLnLi2nhjDC"
      },
      "source": [
        "adam = Adam(learning_rate=0.001)\r\n",
        "cnn = build_model5(left_shape, bn=1, reg=0)\r\n",
        "siamese = models.build_siamese(left_shape, cnn, 0.1)\r\n",
        "siamese.compile(loss='binary_crossentropy', optimizer=adam, metrics=[BinaryAccuracy(name='accuracy')])\r\n",
        "siamese.load_weights(os.path.join(results_path, 'osaka', 'model_r{}_weights.h5'.format(0)))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "68zo_lIziFii",
        "outputId": "b6a9d60a-7cf9-41bd-b676-6b2cfdc0991e"
      },
      "source": [
        "print('Siamese network direct test accuracy:')\r\n",
        "siamese.evaluate([l_input_test, r_input_test], b_labels_test)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Siamese network direct test accuracy:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-6e0bb9ec55de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Siamese network direct test accuracy:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_input_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'l_input_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK0wUaUgTFxn"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "print('Siamese network direct test accuracy:')\n",
        "siamese.evaluate([l_input_test, r_input_test], b_labels_test)\n",
        "\n",
        "print('==========SVM==========')\n",
        "classifier = SVC(kernel='poly', degree=10, C=100)\n",
        "feature_exctractor = Model(inputs=[siamese.get_layer('left_input').input,siamese.get_layer('right_input').input], outputs=siamese.get_layer('lambda').output)\n",
        "d_vect = feature_exctractor.predict([l_input, r_input])\n",
        "d_vect_test = feature_exctractor.predict([l_input_test, r_input_test])\n",
        "print(d_vect.shape)\n",
        "\n",
        "classifier.fit(d_vect, b_labels)\n",
        "svc_score = classifier.score(d_vect_test, b_labels_test)\n",
        "print('SVC mean test accuracy:', svc_score*100)\n",
        "\n",
        "pca = PCA(2)\n",
        "colors = ['#800000',\n",
        "          '#bfef45']\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(121)\n",
        "plt.title('Ground Truth')\n",
        "d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
        "plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in b_labels_test], alpha=0.2)\n",
        "plt.subplot(122)\n",
        "plt.title('Prediction')\n",
        "d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
        "plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in classifier.predict(d_vect_test)], alpha=0.2)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# # print('==========OneClassSVM==========')\n",
        "\n",
        "# # outlier_detector = OneClassSVM(kernel='rbf')\n",
        "# # outlier_detector.fit(d_vect, b_labels)\n",
        "# # test_predictions = np.uint8(outlier_detector.predict(d_vect_test) > 0)\n",
        "# # svc_score = (test_predictions == b_labels_test).mean() * 100\n",
        "# # print('OneClassSVM mean test accuracy:', svc_score)\n",
        "\n",
        "# # pca = PCA(2)\n",
        "# # colors = ['#800000',\n",
        "# #           '#bfef45']\n",
        "# # plt.figure(figsize=(16,6))\n",
        "# # plt.subplot(121)\n",
        "# # plt.title('Ground Truth')\n",
        "# # d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
        "# # plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in b_labels_test], alpha=0.2)\n",
        "# # plt.subplot(122)\n",
        "# # plt.title('Prediction')\n",
        "# # d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
        "# # plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in test_predictions], alpha=0.2)\n",
        "# # plt.show()\n",
        "\n",
        "# print('==========ANN==========')\n",
        "\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# classifier = Sequential()\n",
        "# classifier.add(Dense(8, activation='relu', input_shape=(64,)))\n",
        "# classifier.add(Dropout(0.3))\n",
        "# classifier.add(Dense(8, activation='relu'))\n",
        "# classifier.add(Dropout(0.3))\n",
        "# classifier.add(Dense(1, activation='sigmoid'))\n",
        "# classifier.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# classifier.fit(d_vect, b_labels, verbose=0, epochs=20, batch_size=128)\n",
        "# test_predictions = np.uint8(np.squeeze(classifier.predict(d_vect_test) >= 0.5))\n",
        "# # print(test_predictions[:10])\n",
        "# ann_score = classifier.evaluate(d_vect_test, b_labels_test)[1]\n",
        "# print('ANN mean test accuracy:', ann_score*100)\n",
        "\n",
        "# pca = PCA(2)\n",
        "# colors = ['#800000',\n",
        "#           '#bfef45']\n",
        "# plt.figure(figsize=(16,6))\n",
        "# plt.subplot(121)\n",
        "# plt.title('Ground Truth')\n",
        "# d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
        "# plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in b_labels_test], alpha=0.2)\n",
        "# plt.subplot(122)\n",
        "# plt.title('Prediction')\n",
        "# d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
        "# plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in test_predictions], alpha=0.2)\n",
        "# plt.show()\n",
        "\n",
        "# print('==========KNN==========')\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# knn = KNeighborsClassifier(n_neighbors=5)\n",
        "# knn.fit(d_vect, b_labels)\n",
        "# test_predictions = knn.predict(d_vect_test)\n",
        "# knn_score = knn.score(d_vect_test, b_labels_test)\n",
        "# print('KNN mean test accuracy:', knn_score*100)\n",
        "\n",
        "# pca = PCA(2)\n",
        "# colors = ['#800000',\n",
        "#           '#bfef45']\n",
        "# plt.figure(figsize=(16,6))\n",
        "# plt.subplot(121)\n",
        "# plt.title('Ground Truth')\n",
        "# d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
        "# plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in b_labels_test], alpha=0.2)\n",
        "# plt.subplot(122)\n",
        "# plt.title('Prediction')\n",
        "# d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
        "# plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in test_predictions], alpha=0.2)\n",
        "# plt.show()\n",
        "\n",
        "print('==========GaussionMixture==========')\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gmodel = GaussianMixture(n_components=5)\n",
        "gmodel.fit(d_vect, b_labels)\n",
        "test_predictions =  np.uint8((gmodel.score_samples(d_vect_test) > -55))\n",
        "print(test_predictions)\n",
        "gmodel_score = (test_predictions == b_labels_test).mean()\n",
        "print('Gaussian Mixture mean test accuracy:', gmodel_score*100)\n",
        "\n",
        "pca = PCA(2)\n",
        "colors = ['#800000',\n",
        "          '#bfef45']\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(121)\n",
        "plt.title('Ground Truth')\n",
        "d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
        "plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in b_labels_test], alpha=0.2)\n",
        "plt.subplot(122)\n",
        "plt.title('Prediction')\n",
        "d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
        "plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in test_predictions], alpha=0.2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}