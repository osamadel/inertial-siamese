{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvAor71-TFxU"
   },
   "source": [
    "# IJCNN Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DBFcSs6TFxe"
   },
   "source": [
    "Roughly speaking, I will try to use the models trained on the datasets, then remove the FC layer and add other classifiers. Then I will report the result of the other classifiers on the final performance using either EER or accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cOPS399rTFxe"
   },
   "outputs": [],
   "source": [
    "data_path = '../data/osaka'\n",
    "results_path = '../results/'\n",
    "\n",
    "# data_path = '/content/drive/MyDrive/IJCNN/data/osaka'\n",
    "# results_path = '/content/drive/MyDrive/IJCNN/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OI-lQFRRKEAY",
    "outputId": "7e85a733-61b5-4742-a5c2-2ed5b6502d45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jArT8WzDKPH2",
    "outputId": "e463a28e-25ed-4c96-870b-9cfec0a36b87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze.py  datasets.py  drive\tmodels.py  sample_data\tsrc\n"
     ]
    }
   ],
   "source": [
    "!cp -r \"/content/drive/MyDrive/IJCNN/src\" /content/\n",
    "!mv src/datasets.py datasets.py\n",
    "!mv src/models.py models.py\n",
    "!mv src/analyze.py analyze.py\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k0TFGFVAKTNo"
   },
   "outputs": [],
   "source": [
    "import datasets, models, analyze\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import activations\n",
    "# from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "# import nevergrad as ng\n",
    "import os\n",
    "np.set_printoptions(precision=4)\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qwqf4RGPJ08n"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jvrnb1vgKVPf"
   },
   "outputs": [],
   "source": [
    "def prepare_data(train_sps, val_sps, test_sps, max_cls, split, data, labels, indices_list, run_n):\n",
    "    train_split = int(split[0] * max_cls)\n",
    "    val_split = int(split[2] * max_cls)\n",
    "\n",
    "    rand_indices = np.random.choice(range(max_cls), size=max_cls, replace=False)\n",
    "    indices_list.append(rand_indices)\n",
    "    print('Run #%d:'%run_n)\n",
    "    \n",
    "    all_data_rand, all_labels_rand = [data[x] for x in rand_indices], [labels[x] for x in rand_indices]\n",
    "    b_data, b_labels = datasets.generate_batch(train_sps, all_data_rand[:train_split], all_labels_rand[:train_split])\n",
    "    \n",
    "    val_data, val_labels = datasets.generate_batch(val_sps, all_data_rand[train_split:train_split+val_split], all_labels_rand[train_split:train_split+val_split])\n",
    "    ridx = np.random.choice(range(b_data[0].shape[0]), size=b_data[0].shape[0], replace=False)\n",
    "    \n",
    "    b_data_test, b_labels_test = datasets.generate_batch(test_sps, all_data_rand[train_split+val_split:], all_labels_rand[train_split+val_split:])\n",
    "\n",
    "    l_input = b_data[0][ridx]\n",
    "    r_input = b_data[1][ridx]\n",
    "    b_labels = b_labels[ridx]\n",
    "\n",
    "    # print(l_input[0].shape)\n",
    "\n",
    "    l_input_val = val_data[0]\n",
    "    r_input_val = val_data[1]\n",
    "\n",
    "    l_input_test = b_data_test[0]\n",
    "    r_input_test = b_data_test[1]\n",
    "    return l_input, r_input, b_labels, l_input_val, r_input_val, val_labels, l_input_test, r_input_test, b_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JYx3YqoxK4PG"
   },
   "outputs": [],
   "source": [
    "def load_dataset(segLen, overlap, acc_only):\n",
    "    data, labels = datasets.load_segment_osaka(data_path, \n",
    "                                            [0,744], \n",
    "                                            sample_rate=100,\n",
    "                                            acc_only=acc_only,\n",
    "                                            segment_time=segLen, \n",
    "                                            overlapped=overlap, \n",
    "                                            overlap=overlap, \n",
    "                                            downsample=True)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-h4dUZV8WXlb"
   },
   "outputs": [],
   "source": [
    "def build_model1(input_shape, bn, reg):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv1D, \\\n",
    "                                        MaxPooling1D, \\\n",
    "                                        AveragePooling1D, \\\n",
    "                                        Dense, Flatten, Dropout, \\\n",
    "                                        BatchNormalization, \\\n",
    "                                        GlobalMaxPooling1D, \\\n",
    "                                        GlobalAveragePooling1D, \\\n",
    "                                        Activation\n",
    "    from tensorflow.keras import activations\n",
    "    import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "    model = Sequential()\n",
    "    if reg:\n",
    "        model.add(Conv1D(16, 3, strides=1, input_shape=input_shape, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(16, 3, strides=1, input_shape=input_shape))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    if reg:\n",
    "        model.add(Conv1D(64, 5, strides=2, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(64, 5, strides=2))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Auc0ROjJXJ4a"
   },
   "outputs": [],
   "source": [
    "# def build_model2(input_shape, bn, reg):\n",
    "#     from tensorflow.keras.models import Sequential\n",
    "#     from tensorflow.keras.layers import Conv1D, \\\n",
    "#                                         MaxPooling1D, \\\n",
    "#                                         AveragePooling1D, \\\n",
    "#                                         Dense, Flatten, Dropout, \\\n",
    "#                                         BatchNormalization, \\\n",
    "#                                         GlobalMaxPooling1D, \\\n",
    "#                                         GlobalAveragePooling1D, \\\n",
    "#                                         Activation\n",
    "#     from tensorflow.keras import activations\n",
    "#     import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "#     model = Sequential()\n",
    "#     if reg:\n",
    "#         model.add(Conv1D(16, 3, strides=1, activation='tanh', input_shape=input_shape, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
    "#         model.add(Conv1D(32, 3, strides=2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
    "#     else:\n",
    "#         model.add(Conv1D(16, 3, strides=1, activation='tanh', input_shape=input_shape))\n",
    "#         model.add(Conv1D(32, 3, strides=2, activation='relu'))\n",
    "    \n",
    "#     if bn:\n",
    "#         model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "#     if reg:\n",
    "#         model.add(Conv1D(64, 5, strides=2, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
    "#         model.add(Conv1D(128, 5, strides=3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)))\n",
    "#     else:\n",
    "#         model.add(Conv1D(64, 5, strides=2, activation='tanh'))\n",
    "#         model.add(Conv1D(128, 5, strides=3, activation='relu'))\n",
    "    \n",
    "#     if bn:\n",
    "#         model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(GlobalMaxPooling1D())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1S9BaZbCZajy"
   },
   "outputs": [],
   "source": [
    "# def build_model3(input_shape, bn, reg):\n",
    "#     from tensorflow.keras.models import Sequential\n",
    "#     from tensorflow.keras.layers import Conv1D, \\\n",
    "#                                         MaxPooling1D, \\\n",
    "#                                         AveragePooling1D, \\\n",
    "#                                         Dense, Flatten, Dropout, \\\n",
    "#                                         BatchNormalization, \\\n",
    "#                                         GlobalMaxPooling1D, \\\n",
    "#                                         GlobalAveragePooling1D, \\\n",
    "#                                         Activation\n",
    "#     from tensorflow.keras import activations\n",
    "#     import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv1D(64, 5, strides=3, padding='valid', activation='relu', input_shape=input_shape))\n",
    "#     if bn: model.add(BatchNormalization())\n",
    "#     model.add(Conv1D(128, 3, strides=2, padding='valid', activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "#     model.add(Conv1D(128, 2, strides=1, padding='valid', activation='tanh'))\n",
    "#     model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "#     model.add(GlobalMaxPooling1D())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "q7x9rfwDIqx-"
   },
   "outputs": [],
   "source": [
    "def build_model4(input_shape, bn, reg):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv1D, \\\n",
    "                                        MaxPooling1D, \\\n",
    "                                        AveragePooling1D, \\\n",
    "                                        Dense, Flatten, Dropout, \\\n",
    "                                        BatchNormalization, \\\n",
    "                                        GlobalMaxPooling1D, \\\n",
    "                                        GlobalAveragePooling1D, \\\n",
    "                                        Activation\n",
    "    from tensorflow.keras import activations\n",
    "    import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "    model = Sequential()\n",
    "    if reg:\n",
    "        model.add(Conv1D(32, 3, strides=1, input_shape=input_shape, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(32, 3, strides=1, input_shape=input_shape))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    if reg:\n",
    "        model.add(Conv1D(128, 5, strides=2, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(128, 5, strides=2))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "F28ig3xVIzuJ"
   },
   "outputs": [],
   "source": [
    "def build_model5(input_shape, bn, reg):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv1D, \\\n",
    "                                        MaxPooling1D, \\\n",
    "                                        AveragePooling1D, \\\n",
    "                                        Dense, Flatten, Dropout, \\\n",
    "                                        BatchNormalization, \\\n",
    "                                        GlobalMaxPooling1D, \\\n",
    "                                        GlobalAveragePooling1D, \\\n",
    "                                        Activation\n",
    "    from tensorflow.keras import activations\n",
    "    import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "    model = Sequential()\n",
    "    if reg:\n",
    "        model.add(Conv1D(64, 3, strides=1, input_shape=input_shape, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(64, 3, strides=1, input_shape=input_shape))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    if reg:\n",
    "        model.add(Conv1D(256, 5, strides=2, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                        bias_regularizer=regularizers.l2(1e-4)))\n",
    "    else:\n",
    "        model.add(Conv1D(256, 5, strides=2))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.relu))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DFUOb5_GUo-e"
   },
   "outputs": [],
   "source": [
    "acc_only_list    = [False, True]\n",
    "segLen_list      = [1, 2]\n",
    "overlap_list     = [0, 0.25, 0.5, 0.75]\n",
    "# models_list      = [build_model1, build_model2, build_model3]\n",
    "models_list      = [build_model5]\n",
    "# bn_list          = [True, False]\n",
    "bn_list          = [True]\n",
    "# reg_list         = [True, False]\n",
    "reg_list         = [False]\n",
    "\n",
    "# Configurations\n",
    "train_sps       = 50\n",
    "val_sps         = 20\n",
    "test_sps        = 20\n",
    "max_cls         = 744\n",
    "split           = [0.8, 0.1, 0.1]\n",
    "output_dropout  = 0.1\n",
    "runs            = 10\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CxkqRdMpDOip"
   },
   "outputs": [],
   "source": [
    "def getSiameseAcc(siamese, test_set, EER_th):\n",
    "    # l_input_test, r_input_test = test_set[0]\n",
    "    b_labels_test = test_set[1]\n",
    "    # print('EER threshold:', EER_th)\n",
    "    # print(np.uint8(np.squeeze(siamese.predict(test_set[0])) >= EER_th))\n",
    "    # print(b_labels_test)\n",
    "    return np.mean(np.uint8(np.squeeze(siamese.predict(test_set[0])) >= EER_th) == b_labels_test)\n",
    "\n",
    "def trainSVM(siamese, training_set, test_set):\n",
    "    l_input, r_input = training_set[0]\n",
    "    b_labels = training_set[1]\n",
    "    l_input_test, r_input_test = test_set[0]\n",
    "    b_labels_test = test_set[1]\n",
    "\n",
    "    # classifier = SVC(kernel='poly', degree=10, C=100)\n",
    "    classifier = SVC(kernel='poly', degree=5, C=10)\n",
    "    feature_exctractor = Model(inputs=[siamese.get_layer('left_input').input,siamese.get_layer('right_input').input], outputs=siamese.get_layer('lambda').output)\n",
    "    d_vect = feature_exctractor.predict([l_input, r_input])\n",
    "    d_vect_test = feature_exctractor.predict([l_input_test, r_input_test])\n",
    "    \n",
    "    # print(d_vect.shape)\n",
    "\n",
    "    classifier.fit(d_vect, b_labels)\n",
    "    svc_score = classifier.score(d_vect_test, b_labels_test)\n",
    "    # print('SVC mean test accuracy:', svc_score*100)\n",
    "    return svc_score\n",
    "\n",
    "def trainKNN(siamese, training_set, test_set):\n",
    "    l_input, r_input = training_set[0]\n",
    "    b_labels = training_set[1]\n",
    "    l_input_test, r_input_test = test_set[0]\n",
    "    b_labels_test = test_set[1]\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    feature_exctractor = Model(inputs=[siamese.get_layer('left_input').input,siamese.get_layer('right_input').input], outputs=siamese.get_layer('lambda').output)\n",
    "    d_vect = feature_exctractor.predict([l_input, r_input])\n",
    "    d_vect_test = feature_exctractor.predict([l_input_test, r_input_test])\n",
    "\n",
    "    knn.fit(d_vect, b_labels)\n",
    "    test_predictions = knn.predict(d_vect_test)\n",
    "    knn_score = knn.score(d_vect_test, b_labels_test)\n",
    "    # print('KNN mean test accuracy:', knn_score*100)\n",
    "    return knn_score\n",
    "\n",
    "def trainANN(siamese, training_set, test_set):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "    l_input, r_input = training_set[0]\n",
    "    b_labels = training_set[1]\n",
    "    l_input_test, r_input_test = test_set[0]\n",
    "    b_labels_test = test_set[1]\n",
    "\n",
    "    feature_exctractor = Model(inputs=[siamese.get_layer('left_input').input,siamese.get_layer('right_input').input], outputs=siamese.get_layer('lambda').output)\n",
    "    d_vect = feature_exctractor.predict([l_input, r_input])\n",
    "    d_vect_test = feature_exctractor.predict([l_input_test, r_input_test])\n",
    "\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(8, activation='relu', input_shape=(256,)))\n",
    "    classifier.add(Dropout(0.3))\n",
    "    classifier.add(Dense(8, activation='relu'))\n",
    "    classifier.add(Dropout(0.3))\n",
    "    classifier.add(Dense(1, activation='sigmoid'))\n",
    "    classifier.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "\n",
    "    classifier.fit(d_vect, b_labels, verbose=0, epochs=20, batch_size=128)\n",
    "    # test_predictions = np.uint8(np.squeeze(classifier.predict(d_vect_test) >= 0.5))\n",
    "    # print(test_predictions[:10])\n",
    "    ann_score = classifier.evaluate(d_vect_test, b_labels_test)[1]\n",
    "    # print('ANN mean test accuracy:', ann_score*100)\n",
    "    return ann_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "N-z-EqS3TFxl"
   },
   "outputs": [],
   "source": [
    "def one_run(train_sps, val_sps, test_sps, max_cls, split, data, labels, lst, run_n):\n",
    "    accuracies = {}\n",
    "    l_input, r_input, b_labels \\\n",
    "        , l_input_val, r_input_val, val_labels \\\n",
    "            , l_input_test, r_input_test, b_labels_test = prepare_data( train_sps,\n",
    "                                                                        val_sps,\n",
    "                                                                        test_sps,\n",
    "                                                                        max_cls,\n",
    "                                                                        split,\n",
    "                                                                        data,\n",
    "                                                                        labels,\n",
    "                                                                        [],\n",
    "                                                                        run_n)\n",
    "    print('Loaded data')\n",
    "    adam = Adam(learning_rate=0.001)\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "    \n",
    "    cnn = build_model5(l_input[0].shape, bn=1, reg=0)\n",
    "    siamese = models.build_siamese(l_input[0].shape, cnn, output_dropout)\n",
    "    siamese.compile(loss='binary_crossentropy', optimizer=adam, metrics=[BinaryAccuracy(name='accuracy')])\n",
    "    print('Built model')\n",
    "    if run_n == 0: \n",
    "        cnn.summary()\n",
    "    print('Start training ...')\n",
    "    hist = siamese.fit([l_input, r_input], \n",
    "                        b_labels, \n",
    "                        shuffle=True,\n",
    "                        batch_size=64,\n",
    "                        epochs=100,\n",
    "                        callbacks=[earlystop],\n",
    "                        validation_data=([l_input_val, r_input_val], val_labels),\n",
    "                        verbose=2\n",
    "                        )\n",
    "    print('Finished training')\n",
    "    siamese.save_weights(os.path.join(results_path, 'osaka', 'model_r{}_weights.h5'.format(run_n)))\n",
    "    print('Saved model')\n",
    "    FRR, FAR, EER, EER_th = analyze.ROC(siamese, [l_input_test, r_input_test], b_labels_test)\n",
    "    print('Siamese EER:', EER)\n",
    "    accuracies['siamese'] = getSiameseAcc(siamese, ([l_input_test, r_input_test], b_labels_test), EER_th)\n",
    "    print('Siamese Accuracy:', accuracies['siamese'])\n",
    "    accuracies['svm'] = trainSVM(siamese, ([l_input, r_input], b_labels), ([l_input_test, r_input_test], b_labels_test))\n",
    "    print('SVM Accuracy:', accuracies['svm'])\n",
    "    accuracies['ann'] = trainANN(siamese, ([l_input, r_input], b_labels), ([l_input_test, r_input_test], b_labels_test))\n",
    "    print('ANN Accuracy:', accuracies['ann'])\n",
    "    accuracies['knn'] = trainKNN(siamese, ([l_input, r_input], b_labels), ([l_input_test, r_input_test], b_labels_test))\n",
    "    print('KNN Accuracy:', accuracies['knn'])\n",
    "\n",
    "\n",
    "    return siamese, [FRR, FAR, EER, EER_th], accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7TKW9MyTFxl"
   },
   "source": [
    "### Experiment 1: Train Different Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hiy6B7cuTFxm",
    "outputId": "0d3b8a68-833f-49db-8694-01d1d4aa6146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #0:\n",
      "Loaded data\n",
      "Built model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 98, 64)            1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 98, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 98, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 49, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 23, 256)           82176     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 23, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 23, 256)           0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 84,672\n",
      "Trainable params: 84,032\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "Start training ...\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "data, labels = load_dataset(2, 0.75, acc_only=False)\n",
    "num_cls = 3 # number of classifiers\n",
    "total_accs = np.zeros((runs, num_cls))\n",
    "\n",
    "# Results of this configuration/experiment\n",
    "# EERs = np.array([np.zeros(runs) for _ in range(3*2*2)])\n",
    "\n",
    "for run_n in range(runs):\n",
    "    siamese, ROC, accs = one_run(train_sps, val_sps, test_sps, max_cls, split, data, labels, [], run_n)\n",
    "    for ii, c in enumerate(accs):\n",
    "        total_accs[run_n, ii] = accs[c]\n",
    "    \n",
    "    np.save(os.path.join(results_path, 'osaka', 'classifiers.npy'), total_accs)\n",
    "    \n",
    "#     EERs[4 * model_index + 2*bn + reg , run_n] = EER\n",
    "#     np.save(results_path + 'osaka_filters_eers2.npy', EERs)\n",
    "\n",
    "for ii, c in enumerate(accs):\n",
    "    print(total_accs[:,ii])\n",
    "    print('{} accuracy: {} +/- {}'.format(c, total_accs[:,ii].mean(), total_accs[:,ii].std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "WDLnLi2nhjDC"
   },
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.001)\n",
    "cnn = build_model5(left_shape, bn=1, reg=0)\n",
    "siamese = models.build_siamese(left_shape, cnn, 0.1)\n",
    "siamese.compile(loss='binary_crossentropy', optimizer=adam, metrics=[BinaryAccuracy(name='accuracy')])\n",
    "siamese.load_weights(os.path.join(results_path, 'osaka', 'model_r{}_weights.h5'.format(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "68zo_lIziFii",
    "outputId": "b6a9d60a-7cf9-41bd-b676-6b2cfdc0991e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese network direct test accuracy:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-6e0bb9ec55de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Siamese network direct test accuracy:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_input_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'l_input_test' is not defined"
     ]
    }
   ],
   "source": [
    "print('Siamese network direct test accuracy:')\n",
    "siamese.evaluate([l_input_test, r_input_test], b_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iK0wUaUgTFxn"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "print('Siamese network direct test accuracy:')\n",
    "siamese.evaluate([l_input_test, r_input_test], b_labels_test)\n",
    "\n",
    "print('==========SVM==========')\n",
    "classifier = SVC(kernel='poly', degree=10, C=100)\n",
    "feature_exctractor = Model(inputs=[siamese.get_layer('left_input').input,siamese.get_layer('right_input').input], outputs=siamese.get_layer('lambda').output)\n",
    "d_vect = feature_exctractor.predict([l_input, r_input])\n",
    "d_vect_test = feature_exctractor.predict([l_input_test, r_input_test])\n",
    "print(d_vect.shape)\n",
    "\n",
    "classifier.fit(d_vect, b_labels)\n",
    "svc_score = classifier.score(d_vect_test, b_labels_test)\n",
    "print('SVC mean test accuracy:', svc_score*100)\n",
    "\n",
    "pca = PCA(2)\n",
    "colors = ['#800000',\n",
    "          '#bfef45']\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "plt.title('Ground Truth')\n",
    "d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
    "plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in b_labels_test], alpha=0.2)\n",
    "plt.subplot(122)\n",
    "plt.title('Prediction')\n",
    "d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
    "plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in classifier.predict(d_vect_test)], alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # print('==========OneClassSVM==========')\n",
    "\n",
    "# # outlier_detector = OneClassSVM(kernel='rbf')\n",
    "# # outlier_detector.fit(d_vect, b_labels)\n",
    "# # test_predictions = np.uint8(outlier_detector.predict(d_vect_test) > 0)\n",
    "# # svc_score = (test_predictions == b_labels_test).mean() * 100\n",
    "# # print('OneClassSVM mean test accuracy:', svc_score)\n",
    "\n",
    "# # pca = PCA(2)\n",
    "# # colors = ['#800000',\n",
    "# #           '#bfef45']\n",
    "# # plt.figure(figsize=(16,6))\n",
    "# # plt.subplot(121)\n",
    "# # plt.title('Ground Truth')\n",
    "# # d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
    "# # plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in b_labels_test], alpha=0.2)\n",
    "# # plt.subplot(122)\n",
    "# # plt.title('Prediction')\n",
    "# # d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
    "# # plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in test_predictions], alpha=0.2)\n",
    "# # plt.show()\n",
    "\n",
    "# print('==========ANN==========')\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# classifier = Sequential()\n",
    "# classifier.add(Dense(8, activation='relu', input_shape=(64,)))\n",
    "# classifier.add(Dropout(0.3))\n",
    "# classifier.add(Dense(8, activation='relu'))\n",
    "# classifier.add(Dropout(0.3))\n",
    "# classifier.add(Dense(1, activation='sigmoid'))\n",
    "# classifier.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "# classifier.fit(d_vect, b_labels, verbose=0, epochs=20, batch_size=128)\n",
    "# test_predictions = np.uint8(np.squeeze(classifier.predict(d_vect_test) >= 0.5))\n",
    "# # print(test_predictions[:10])\n",
    "# ann_score = classifier.evaluate(d_vect_test, b_labels_test)[1]\n",
    "# print('ANN mean test accuracy:', ann_score*100)\n",
    "\n",
    "# pca = PCA(2)\n",
    "# colors = ['#800000',\n",
    "#           '#bfef45']\n",
    "# plt.figure(figsize=(16,6))\n",
    "# plt.subplot(121)\n",
    "# plt.title('Ground Truth')\n",
    "# d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
    "# plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in b_labels_test], alpha=0.2)\n",
    "# plt.subplot(122)\n",
    "# plt.title('Prediction')\n",
    "# d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
    "# plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in test_predictions], alpha=0.2)\n",
    "# plt.show()\n",
    "\n",
    "# print('==========KNN==========')\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# knn.fit(d_vect, b_labels)\n",
    "# test_predictions = knn.predict(d_vect_test)\n",
    "# knn_score = knn.score(d_vect_test, b_labels_test)\n",
    "# print('KNN mean test accuracy:', knn_score*100)\n",
    "\n",
    "# pca = PCA(2)\n",
    "# colors = ['#800000',\n",
    "#           '#bfef45']\n",
    "# plt.figure(figsize=(16,6))\n",
    "# plt.subplot(121)\n",
    "# plt.title('Ground Truth')\n",
    "# d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
    "# plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in b_labels_test], alpha=0.2)\n",
    "# plt.subplot(122)\n",
    "# plt.title('Prediction')\n",
    "# d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
    "# plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in test_predictions], alpha=0.2)\n",
    "# plt.show()\n",
    "\n",
    "print('==========GaussionMixture==========')\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmodel = GaussianMixture(n_components=5)\n",
    "gmodel.fit(d_vect, b_labels)\n",
    "test_predictions =  np.uint8((gmodel.score_samples(d_vect_test) > -55))\n",
    "print(test_predictions)\n",
    "gmodel_score = (test_predictions == b_labels_test).mean()\n",
    "print('Gaussian Mixture mean test accuracy:', gmodel_score*100)\n",
    "\n",
    "pca = PCA(2)\n",
    "colors = ['#800000',\n",
    "          '#bfef45']\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "plt.title('Ground Truth')\n",
    "d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
    "plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in b_labels_test], alpha=0.2)\n",
    "plt.subplot(122)\n",
    "plt.title('Prediction')\n",
    "d_vect_test_2d = pca.fit_transform(d_vect_test)\n",
    "plt.scatter(d_vect_test_2d[:, 0], d_vect_test_2d[:, 1], c=[colors[i] for i in test_predictions], alpha=0.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "osaka_cls_retrain.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
